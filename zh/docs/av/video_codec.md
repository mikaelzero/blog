视频中图像一般是 YUV 格式。现在，我们假设有一个电影视频，分辨率是 1080P，帧率是 25fps，并且时长是 2 小时，如果不做视频压缩的话，它的大小是 1920 x 1080 x 1.5 x 25 x 2 x 3600 = 260.7G,所以，我们需要对视频进行编码压缩

## 视频为什么可以压缩？(数据冗余)

图像一般都是有数据冗余的，主要包括以下 4 种:

- 空间冗余。视频的某一帧内部的相邻像素存在相似性
- 时间冗余。一个帧率为 25fps 的视频中前后两帧图像相差只有 40ms，两张图像的变化 是比较小的，相似性很高，这种叫做时间冗余。
- 视觉冗余。我们的眼睛是有视觉灵敏度这个东西的。人的眼睛对于图像中高频信息的敏 感度是小于低频信息的。有的时候去除图像中的一些高频信息，人眼看起来跟不去除高 频信息差别不大，这种叫做视觉冗余。
- 信息熵冗余。我们一般会使用 Zip 等压缩工具去压缩文件，将文件大小减小，这个对于 图像来说也是可以做的，这种冗余叫做信息熵冗余。视频编码就是通过减少上述 4 种冗余来达到压缩视频的目的。

![](https://raw.githubusercontent.com/mikaelzero/ImageSource/main/uPic/p8pKPV.png)

## 视频编码的原理

一般我们所熟知的彩色图像的格式是 RGB 的。但是，RGB 三个颜色是有相关性的，为了去掉这个相关性，减少需要编码的信息量，我们通常会把 RGB 转换成 YUV

相关性是指一幅图像在 RGB 格式的时候，将 R、G、B 三个通道分离开来当作图像来看的话，R、G、B 三张图像内容几乎是一样的，只是颜色不同而已。具有相关性，如果拿来编码的话，三张图像同等重要，而且轮廓还差不多，但颜色又不同，因此不好编码。而 YUV 不同，YUV 中只有 Y 是图像的大体轮廓，没有颜色信息。U、V 是颜色信息。三张图像相互独立。并且人眼对于色彩信息相比图像的轮廓信息不敏感些。我们可以缩小 U、V 的大小，比如 YUV420 中 U、V 只有 Y 的 1/4 大小，本身就相比于 RGB 图像小了一半。然后我们编码的时候 Y、U、V 相关性很小，可以独立编码，也很方便。

视频压缩算法要做两件重要的事（如在 H.264/AVC、HEVC、VP9 和 AV1 等编解码器中）：

使用 DCT（Discrete Cosine Transform，离散余弦变换）将“像素域”转换为“频域”。如果你还不了解什么是 DCT，可以看下这篇文章（[如何给 5 岁孩子解释 DCT？](https://mp.weixin.qq.com/s?__biz=MzU1NTEzOTM5Mw==&mid=2247513696&idx=1&sn=1ed871393a85e35177879cf6eb91ce3b&chksm=fbda1e4eccad9758e2883f178e09f9a225cdae38b7fd5c92527cabe0f71adfba5ab28a62a76f&scene=21#wechat_redirect)）。

通过一种被称为量化的技术舍弃一些频域数据（被称为系数），同时确保人眼无法感知这种数据丢失。

如果一个视频是纯色的且不会改变，在压缩的时候，你会怎么设计？当然是只存一个颜色值，然后其他位置都用同一个颜色值；如果一个视频只有一部分在改变，那就找到一个算法，来减少不动区域的开销。所谓编码算法，就是寻找规律，构建模型。谁能找到更精准的规律，建立更高效的模型，谁就是厉害的算法。

## GOP

将一串连续的相似的帧归到一个图像群组（Group Of Pictures，GOP）

GOP 过长的缺点:

- 加大 GOP 长度有利于减小视频文件大小，但也不宜设置过大，太大则会导致 GOP 后部帧的画面失真，影响视频质量
- 由于 P、B 帧的复杂度大于 I 帧，GOP 值过大，过多的 P、B 帧会影响编码效率，使编码效率降低
- GOP 长度也是影响视频 seek 响应速度的关键因素，seek 时播放器需要定位到离指定位置最近的前一个 I 帧，如果 GOP 太大意味着距离指定位置可能越远（需要解码的参考帧就越多）、seek 响应的时间（缓冲时间）也越长

## 宏块

将一副图像数据，划分为 n\*n 大小的网格，每个网格称为宏块

## 视频编码后的结果

**I 帧**

- （I Picture、I Frame、 Intra Coded Picture），译为：帧内编码图像，也叫关键帧（Keyframe）
- 它是视频的第一帧，也是 GOP 的第一帧
- 编码：对整帧图片数据进行编码
- 解码：仅用当前 I 帧的编码数据就可以解码出完整的图像
- 是一种自带全部信息的独立帧，无需参考其他图像便可以独立进行解码，可以简单理解为一张静态图像

![](https://raw.githubusercontent.com/mikaelzero/ImageSource/main/uPic/rkLnOQ.jpg)

**P 帧(预测)**

P 帧（P Picture、P Frame、Predictive Coded Picture），翻译为：预测编码图像，当前的画面几乎总能使用之前的一帧进行渲染

例如，在第二帧，唯一的改变是球向前移动了。仅仅使用（第二帧）对前一帧的引用和差值，我们就能重建前一帧。
![](https://raw.githubusercontent.com/mikaelzero/ImageSource/main/uPic/RYz2uC.jpg) <- ![](https://raw.githubusercontent.com/mikaelzero/ImageSource/main/uPic/wd71H9.jpg)

- 编码：
  - 并不会对整帧图像数据进行编码
  - 以前面的 I 帧或 P 帧作为参考帧，只编码当前 P 帧与参考帧的差异数据
- 解码：需要先解码出前面的参考帧，再结合差异数据解码出当前 P 帧完整的图像

> 既然 P 帧使用较少的数据，为什么我们不能用单个 I 帧和其余的 P 帧来编码整个视频？
> 编码完这个视频之后，开始观看它，并快进到视频的末尾部分，你会注意到它需要花一些时间才真正跳转到这部分。> 这是因为 P 帧需要一个引用帧（比如 I 帧）才能渲染。

**B 帧（双向预测）**

- 双向预测内插编码帧（bi-directionalinterpolated prediction frame）,保存差异信息的百分比，也称为双向预测帧.
- 它的解码依赖前面的 I 帧和后面的 P 帧
- 当帧数据与最近的 I 帧相似程度高于 70%时，编码成 B 帧。
- 硬编码中部分手机关闭了 B 帧的编码，它编码耗时较长。

## 时间冗余（帧间预测）

## 帧内预测

帧内编码/预测用于解决单帧空间冗余问题。如果我们分析视频的每一帧，会发现许多区域是相互关联的。

![](https://raw.githubusercontent.com/mikaelzero/ImageSource/main/uPic/08Jqm2.jpg)

## 简单的编码例子

![](https://raw.githubusercontent.com/mikaelzero/ImageSource/main/uPic/U06QB4.jpg)

- 第一帧数据到来，经过信源编码器，复合编码器，传输编码器被编码成 I 帧，输出到文件中
- 第二帧由于与第一帧非常相似，因此被交给复合编码器编码成 B 帧，进入到传输缓冲器，不会进入传输编码器
- 第三帧同样由于差异低于 30%，被编码成 B 帧进入传输缓冲器，不会输出到文件
- 第四帧由于差异高于 30%，编码成 P 帧，输出到文件，同时通知传输缓冲器，将 B 帧输出到文件。
- 在每个 GOP 编码结果文件中首帧一定是 I 帧，第二帧一定是 P 帧，所以输出一般都是 IPBBBBB

## 总结

视频编码主要分 为熵编码、预测、DCT 变换和量化这几个步骤。

1. 熵编码(以行程编码为例):视频编码中真正实现“压缩”的步骤，主要去除信息熵冗 余。在出现连续多个 0 像素的时候压缩率会更高。
2. 帧内预测:为了提高熵编码的压缩率，先将当前编码块的相邻块像素经过帧内预测算法 得到帧内预测块，再用当前编码块减去帧内预测块得到残差块，从而去掉空间冗余。
3. 帧间预测:类似于帧内预测，在已经编码完成的帧中，先通过运动搜索得到帧间预测 块，再与编码块相减得到残差块，从而去除时间冗余。
4. DCT 变换和量化:将残差块变换到频域，分离高频和低频信息。由于高频信息数量多但 大小相对较小，又人眼对高频信息相对不敏感，我们利用这个特点，使用 QStep 对 DCT 系数进行量化，将大部分高频信息量化为 0，达到去除视觉冗余的目的。

参考：

https://github.com/leandromoreira/digital_video_introduction/blob/master/README-cn.md

https://bbs.huaweicloud.com/blogs/389073

https://juejin.cn/post/7112225628211904549
